{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from datetime import date, timedelta\n",
    "import configparser\n",
    "\n",
    "cf = configparser.ConfigParser()\n",
    "cf.read('config.cfg')\n",
    "                               \n",
    "DB_IP = cf.get('db', 'DB_IP')\n",
    "DB_USER = cf.get('db', 'DB_USER')\n",
    "DB_PWD = cf.get('db', 'DB_PWD')\n",
    "DB_SCH = cf.get('db', 'DB_SCH')\n",
    "\n",
    "LIMIT_FILTER = 0.70\n",
    "\n",
    "INPUT_VEC_SIZE = LSTM_SIZE = 7\n",
    "TIME_STEP_SIZE = 60\n",
    "LABEL_SIZE = 3\n",
    "LSTM_DEPTH = 4\n",
    "\n",
    "BATCH_SIZE = 15000\n",
    "TRAIN_CNT = 100\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBManager :\n",
    "    def __init__(self):\n",
    "        self.conn = self.get_new_conn()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.conn.close()\n",
    "    def get_new_conn(self):\n",
    "        return pymysql.connect(host=DB_IP, user=DB_USER, password=DB_PWD, db=DB_SCH, charset='utf8mb4')\n",
    "    def get_codedates(self, code, limit):    \n",
    "        query = \"SELECT date FROM data.daily_stock WHERE code = %s AND date <= %s ORDER BY date ASC\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query, (code, limit))\n",
    "        code_dates = list()        \n",
    "        dates = cursor.fetchall()\n",
    "        for date in dates:\n",
    "            code_dates.append((code, date[0]))\n",
    "        return code_dates\n",
    "    def get_items(self, code, date, limit):\n",
    "        query = \"SELECT open, high, low, close, volume, hold_foreign, st_purchase_inst FROM data.daily_stock WHERE code = %s AND date >= %s ORDER BY date ASC LIMIT %s\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query, (code, date, limit))\n",
    "        items = cursor.fetchall()        \n",
    "        return items\n",
    "    \n",
    "    def get_codes(self):\n",
    "        query = \"SELECT DISTINCT code FROM data.daily_stock\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    def insert_result(self, expect, code, analyze_at, potential, evaluate, volume) :\n",
    "        if self.check_exist(expect, code, analyze_at, evaluate):\n",
    "            print('duplicate', expect, code, analyze_at)\n",
    "        else :\n",
    "            cursor = self.conn.cursor()\n",
    "            print(expect,code,analyze_at,potential,volume,evaluate)\n",
    "            cursor.execute(\"INSERT INTO forecast (type, code, analyzeAt, potential, volume, evaluate) VALUES (%s, %s, %s, %s, %s, %s)\",\n",
    "                           (expect, code, analyze_at, str(potential), volume, evaluate))\n",
    "            self.conn.commit()\n",
    "    def check_exist(self, expect, code, analyze_at, evaluate):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT count(*) as cnt FROM forecast WHERE type = %s AND code = %s AND analyzeAt = %s AND evaluate = %s\", (expect, code, analyze_at, evaluate))\n",
    "        cnt = cursor.fetchone()\n",
    "        return cnt[0] > 0\n",
    "    def get_volume(self, code, limit_at):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT count(*) as cnt FROM daily_stock WHERE code = %s AND date <= %s\", (code, limit_at))\n",
    "        cnt = cursor.fetchone()\n",
    "        return cnt[0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(code, X, W, B, lstm_size):\n",
    "    XT = tf.transpose(X, [1, 0, 2]) \n",
    "    XR = tf.reshape(XT, [-1, lstm_size])\n",
    "    X_split = tf.split(0, TIME_STEP_SIZE, XR)\n",
    "    with tf.variable_scope(code, reuse=False):\n",
    "        cell = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell = cell, output_keep_prob = 0.5)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * LSTM_DEPTH, state_is_tuple = True)\n",
    "\n",
    "    outputs, _states = tf.nn.rnn(cell, X_split, dtype=tf.float32)\n",
    "\n",
    "    return tf.matmul(outputs[-1], W) + B, cell.state_size # State size to initialize the stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_series_datas(db, code_dates):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for code_date in code_dates:\n",
    "        items = db.get_items(code_date[0], code_date[1], TIME_STEP_SIZE + EVALUATE_SIZE)\n",
    "  \n",
    "        if len(items) < (EVALUATE_SIZE + TIME_STEP_SIZE):\n",
    "            break\n",
    "        X.append(np.array(items[:TIME_STEP_SIZE]))\n",
    "\n",
    "        st_purchase_inst = items[-(EVALUATE_SIZE + 1)][EXPECT]\n",
    "        if st_purchase_inst == 0:\n",
    "            continue\n",
    "        for i in range(EVALUATE_SIZE, len(items) - EVALUATE_SIZE):\n",
    "            eval_inst = items[i][EXPECT]\n",
    "            eval_bef = items[EVALUATE_SIZE-i][EXPECT]\n",
    "            if eval_bef < eval_inst:\n",
    "                eval_bef = eval_inst           \n",
    "        \n",
    "        if (eval_bef - st_purchase_inst) / st_purchase_inst < -0.02: #percent ? cnt ? \n",
    "            Y.append((0., 0., 1.))\n",
    "        elif (eval_bef - st_purchase_inst) / st_purchase_inst > 0.03:\n",
    "            Y.append((1., 0., 0.))\n",
    "        else:\n",
    "            Y.append((0., 1., 0.))\n",
    "\n",
    "\n",
    "    arrX = np.array(X)    \n",
    "    meanX = np.mean(arrX, axis = 0)\n",
    "    stdX = np.std(arrX, axis = 0)\n",
    "    norX = (arrX - meanX) / stdX\n",
    "    norY = np.array(Y)\n",
    "    return norX, norY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_datas(db, code_dates):    \n",
    "    np.random.seed()\n",
    "    np.random.shuffle(code_dates)\n",
    "\n",
    "    trX = list()\n",
    "    trY = list()\n",
    "    trX, trY = read_series_datas(db, code_dates)\n",
    "    teX, teY = read_series_datas(db, code_dates)\n",
    "\n",
    "    return trX, trY, teX, teY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def analyze(code, limit):      \n",
    "    db = DBManager()\n",
    "    code_dates = db.get_codedates(code, limit)\n",
    "    tf.reset_default_graph()    \n",
    "    last = code_dates[-1][1]\n",
    "    trX, trY, teX, teY = read_datas(db, code_dates)\n",
    "    if (len(trX) == 0):\n",
    "        return None\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, TIME_STEP_SIZE, INPUT_VEC_SIZE])\n",
    "    Y = tf.placeholder(tf.float32, [None, LABEL_SIZE])\n",
    "\n",
    "    W = init_weights([LSTM_SIZE, LABEL_SIZE])\n",
    "    B = init_weights([LABEL_SIZE])\n",
    "\n",
    "    py_x, state_size = model(code, X, W, B, LSTM_SIZE)\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(py_x, Y)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "    predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "    # Launch the graph in a session\n",
    "    analyzed = None\n",
    "    with tf.Session() as sess:\n",
    "        # you need to initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for loop in range(TRAIN_CNT):\n",
    "            for start, end in zip(range(0, len(trX), BATCH_SIZE), range(BATCH_SIZE, len(trX)+1, BATCH_SIZE)):\n",
    "                sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "\n",
    "            test_indices = np.arange(len(teY))\n",
    "            org = teY[test_indices] ## fixfix\n",
    "            res = sess.run(predict_op, feed_dict={X: teX[test_indices], Y: teY[test_indices]})\n",
    "            \n",
    "            if loop == TRAIN_CNT-1 :\n",
    "                result = np.mean(np.argmax(org, axis=1) == res)                \n",
    "                analyzed = {\"code\":code, \"per\":round(result, 2), \"date\":limit}\n",
    "                print(analyzed)\n",
    "    return analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/ipykernel/__main__.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2017-02-19', 'code': 'A000030', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A000050', 'per': 0.12}\n",
      "{'date': '2017-02-19', 'code': 'A000070', 'per': 0.40999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A000080', 'per': 0.16}\n",
      "{'date': '2017-02-19', 'code': 'A000100', 'per': 0.23000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A000120', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A000140', 'per': 0.34000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A000150', 'per': 0.65000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A000210', 'per': 0.27000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A000230', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A000240', 'per': 0.25}\n",
      "{'date': '2017-02-19', 'code': 'A000270', 'per': 0.34999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A000640', 'per': 0.34999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A000660', 'per': 0.55000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A000670', 'per': 0.27000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A000720', 'per': 0.55000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A000810', 'per': 0.64000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A000880', 'per': 0.12}\n",
      "{'date': '2017-02-19', 'code': 'A000990', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A001040', 'per': 0.54000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A001060', 'per': 0.33000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A001120', 'per': 0.67000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A001230', 'per': 0.14999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A001430', 'per': 0.25}\n",
      "{'date': '2017-02-19', 'code': 'A001450', 'per': 0.29999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A001520', 'per': 0.14999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A001680', 'per': 0.56000000000000005}\n",
      "{'date': '2017-02-19', 'code': 'A001740', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A001780', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A001800', 'per': 0.28999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A002240', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A002270', 'per': 0.20000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A002350', 'per': 0.34000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A002380', 'per': 0.33000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A002620', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A002790', 'per': 0.57999999999999996}\n",
      "{'date': '2017-02-19', 'code': 'A002960', 'per': 0.11}\n",
      "{'date': '2017-02-19', 'code': 'A003000', 'per': 0.53000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A003030', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A003200', 'per': 0.16}\n",
      "{'date': '2017-02-19', 'code': 'A003240', 'per': 0.28000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A003300', 'per': 0.20999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A003410', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A003490', 'per': 0.12}\n",
      "{'date': '2017-02-19', 'code': 'A003520', 'per': 0.40999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A003550', 'per': 0.17000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A003570', 'per': 0.13}\n",
      "{'date': '2017-02-19', 'code': 'A003620', 'per': 0.14000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A003850', 'per': 0.17999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A003920', 'per': 1.0}\n",
      "6 A003920 2017-02-19 1.0 3258 3\n",
      "insert result  {'date': '2017-02-19', 'code': 'A003920', 'per': 1.0} 3258\n",
      "{'date': '2017-02-19', 'code': 'A004000', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A004020', 'per': 0.34000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A004170', 'per': 0.56000000000000005}\n",
      "{'date': '2017-02-19', 'code': 'A004370', 'per': 0.51000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A004490', 'per': 0.29999999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.4/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python3.4/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.4/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2017-02-19', 'code': 'A004710', 'per': 0.029999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A004800', 'per': 0.22}\n",
      "{'date': '2017-02-19', 'code': 'A004990', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A005090', 'per': 0.44}\n",
      "{'date': '2017-02-19', 'code': 'A005180', 'per': 0.37}\n",
      "{'date': '2017-02-19', 'code': 'A005300', 'per': 0.11}\n",
      "{'date': '2017-02-19', 'code': 'A005380', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A005440', 'per': 0.28999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A005490', 'per': 0.14999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A005610', 'per': 0.32000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A005740', 'per': 0.42999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A005830', 'per': 0.089999999999999997}\n",
      "{'date': '2017-02-19', 'code': 'A005850', 'per': 0.54000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A005930', 'per': 0.10000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A005940', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A006120', 'per': 0.28999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A006260', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A006280', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A006400', 'per': 0.44}\n",
      "{'date': '2017-02-19', 'code': 'A006650', 'per': 0.32000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A006800', 'per': 0.39000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A007070', 'per': 0.64000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A007210', 'per': 0.19}\n",
      "{'date': '2017-02-19', 'code': 'A007310', 'per': 0.28999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A007340', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A007570', 'per': 0.65000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A007690', 'per': 0.35999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A008060', 'per': 0.23000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A008490', 'per': 0.38}\n",
      "{'date': '2017-02-19', 'code': 'A008560', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A008770', 'per': 0.13}\n",
      "{'date': '2017-02-19', 'code': 'A008930', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A009150', 'per': 0.62}\n",
      "{'date': '2017-02-19', 'code': 'A009240', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A009290', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A009420', 'per': 0.35999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A009540', 'per': 0.059999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A009830', 'per': 0.13}\n",
      "{'date': '2017-02-19', 'code': 'A010060', 'per': 0.33000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A010120', 'per': 0.63}\n",
      "{'date': '2017-02-19', 'code': 'A010130', 'per': 0.23000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A010140', 'per': 0.12}\n",
      "{'date': '2017-02-19', 'code': 'A010620', 'per': 0.35999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A010780', 'per': 0.29999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A010950', 'per': 0.42999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A011070', 'per': 0.52000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A011170', 'per': 0.089999999999999997}\n",
      "{'date': '2017-02-19', 'code': 'A011210', 'per': 0.050000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A011780', 'per': 0.27000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A011790', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A012330', 'per': 0.52000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A012450', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A012630', 'per': 1.0}\n",
      "6 A012630 2017-02-19 1.0 3258 3\n",
      "insert result  {'date': '2017-02-19', 'code': 'A012630', 'per': 1.0} 3258\n",
      "{'date': '2017-02-19', 'code': 'A012750', 'per': 0.41999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A014680', 'per': 0.66000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A014820', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A014830', 'per': 0.48999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A015760', 'per': 0.029999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A016360', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A017670', 'per': 0.38}\n",
      "{'date': '2017-02-19', 'code': 'A017800', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A018260', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A018880', 'per': 0.22}\n",
      "{'date': '2017-02-19', 'code': 'A019680', 'per': 0.56000000000000005}\n",
      "{'date': '2017-02-19', 'code': 'A020000', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A020150', 'per': 0.33000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A021240', 'per': 0.14000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A023530', 'per': 0.59999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A024110', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A025540', 'per': 0.23999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A025860', 'per': 0.58999999999999997}\n",
      "{'date': '2017-02-19', 'code': 'A027410', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A028050', 'per': 0.19}\n",
      "{'date': '2017-02-19', 'code': 'A028260', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A029530', 'per': 0.55000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A029780', 'per': 0.20999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A030000', 'per': 0.44}\n",
      "{'date': '2017-02-19', 'code': 'A030200', 'per': 0.26000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A032640', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A032830', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A033780', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A033920', 'per': 0.53000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A034020', 'per': 0.39000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A034120', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A034220', 'per': 0.39000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A034730', 'per': 0.17000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A035250', 'per': 0.14000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A035420', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A036460', 'per': 0.080000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A036570', 'per': 0.38}\n",
      "{'date': '2017-02-19', 'code': 'A036580', 'per': 0.080000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A042660', 'per': 0.14000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A042670', 'per': 0.40000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A047040', 'per': 0.27000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A047050', 'per': 0.26000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A047810', 'per': 0.29999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A049770', 'per': 0.070000000000000007}\n",
      "{'date': '2017-02-19', 'code': 'A051600', 'per': 0.52000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A051900', 'per': 0.42999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A051910', 'per': 0.35999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A052690', 'per': 0.20000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A055550', 'per': 0.58999999999999997}\n",
      "{'date': '2017-02-19', 'code': 'A057050', 'per': 0.080000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A060980', 'per': 0.28000000000000003}\n",
      "{'date': '2017-02-19', 'code': 'A064350', 'per': 0.14000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A064960', 'per': 0.14999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A066570', 'per': 0.22}\n",
      "{'date': '2017-02-19', 'code': 'A069260', 'per': 0.11}\n",
      "{'date': '2017-02-19', 'code': 'A069620', 'per': 0.10000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A069960', 'per': 0.46999999999999997}\n",
      "{'date': '2017-02-19', 'code': 'A071050', 'per': 0.22}\n",
      "{'date': '2017-02-19', 'code': 'A073240', 'per': 0.25}\n",
      "{'date': '2017-02-19', 'code': 'A078520', 'per': 0.20000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A078930', 'per': 0.28999999999999998}\n",
      "{'date': '2017-02-19', 'code': 'A079430', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A086280', 'per': 0.10000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A086790', 'per': 0.25}\n",
      "{'date': '2017-02-19', 'code': 'A088350', 'per': 0.69999999999999996}\n",
      "{'date': '2017-02-19', 'code': 'A090430', 'per': 0.27000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A093050', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A093370', 'per': 0.62}\n",
      "{'date': '2017-02-19', 'code': 'A096770', 'per': 0.12}\n",
      "{'date': '2017-02-19', 'code': 'A097230', 'per': 0.46000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A103140', 'per': 0.57999999999999996}\n",
      "{'date': '2017-02-19', 'code': 'A104700', 'per': 0.62}\n",
      "{'date': '2017-02-19', 'code': 'A105560', 'per': 0.67000000000000004}\n",
      "{'date': '2017-02-19', 'code': 'A105630', 'per': 0.26000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A108670', 'per': 0.33000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A111770', 'per': 0.56999999999999995}\n",
      "{'date': '2017-02-19', 'code': 'A114090', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A115390', 'per': 0.32000000000000001}\n",
      "{'date': '2017-02-19', 'code': 'A120110', 'per': 1.0}\n",
      "6 A120110 2017-02-19 1.0 1746 3\n",
      "insert result  {'date': '2017-02-19', 'code': 'A120110', 'per': 1.0} 1746\n",
      "{'date': '2017-02-19', 'code': 'A128940', 'per': 0.0}\n",
      "{'date': '2017-02-19', 'code': 'A139480', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A145990', 'per': 0.35999999999999999}\n",
      "{'date': '2017-02-19', 'code': 'A161390', 'per': 0.34000000000000002}\n",
      "{'date': '2017-02-19', 'code': 'A161890', 'per': 0.25}\n",
      "{'date': '2017-02-19', 'code': 'A170900', 'per': 1.0}\n",
      "6 A170900 2017-02-19 1.0 953 3\n",
      "insert result  {'date': '2017-02-19', 'code': 'A170900', 'per': 1.0} 953\n",
      "{'date': '2017-02-19', 'code': 'A185750', 'per': 0.31}\n",
      "{'date': '2017-02-19', 'code': 'A192400', 'per': 0.5}\n",
      "{'date': '2017-02-19', 'code': 'A192820', 'per': 0.62}\n",
      "{'date': '2017-02-19', 'code': 'A204320', 'per': 0.5}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "limit = '2017-02-21'\n",
    "EXPECT = 6 ##open, high, low, close, volume, hold_foreign, st_purchase_inst\n",
    "EVALUATE_SIZE = 3\n",
    "\n",
    "codes = DBManager().get_codes()\n",
    "for code in codes : \n",
    "    analyzed = analyze(code[0], limit)\n",
    "    if analyzed is not None and analyzed[\"per\"] > LIMIT_FILTER:\n",
    "        db = DBManager()\n",
    "        volume = db.get_volume(analyzed[\"code\"], limit)\n",
    "        db.insert_result(EXPECT, analyzed[\"code\"], limit, analyzed[\"per\"], EVALUATE_SIZE, volume)        \n",
    "        print('insert result ', analyzed, volume)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
