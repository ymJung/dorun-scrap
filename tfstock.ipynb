{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import configparser\n",
    "\n",
    "cf = configparser.ConfigParser()\n",
    "cf.read('config.cfg')\n",
    "                               \n",
    "DB_IP = cf.get('db', 'DB_IP')\n",
    "DB_USER = cf.get('db', 'DB_USER')\n",
    "DB_PWD = cf.get('db', 'DB_PWD')\n",
    "DB_SCH = cf.get('db', 'DB_SCH')\n",
    "\n",
    "LIMIT_FILTER = 0.70\n",
    "\n",
    "INPUT_VEC_SIZE = 7\n",
    "LSTM_SIZE = 7\n",
    "TIME_STEP_SIZE = 60\n",
    "LABEL_SIZE = 3\n",
    "LSTM_DEPTH = 4\n",
    "EVALUATE_SIZE = 3\n",
    "\n",
    "BATCH_SIZE = 15000\n",
    "TRAIN_CNT = 600\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DBManager :\n",
    "    def __init__(self):\n",
    "        self.conn = self.get_new_conn()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.conn.close()\n",
    "    def get_new_conn(self):\n",
    "        return pymysql.connect(host=DB_IP, user=DB_USER, password=DB_PWD, db=DB_SCH, charset='utf8mb4')\n",
    "    def get_codedates(self, code, limit):    \n",
    "        query = \"SELECT date FROM data.daily_stock WHERE code = %s AND date <= %s ORDER BY date ASC\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query, (code, limit))\n",
    "        code_dates = list()        \n",
    "        dates = cursor.fetchall()\n",
    "        for date in dates:\n",
    "            code_dates.append((code, date[0]))\n",
    "        return code_dates\n",
    "    def get_items(self, code, date, limit):\n",
    "        query = \"SELECT open, high, low, close, volume, hold_foreign, st_purchase_inst FROM data.daily_stock WHERE code = %s AND date >= %s ORDER BY date ASC LIMIT %s\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query, (code, date, limit))\n",
    "        items = cursor.fetchall()        \n",
    "        return items\n",
    "    \n",
    "    def get_codes(self):\n",
    "        query = \"SELECT DISTINCT code FROM data.daily_stock\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall()\n",
    "    def insert_result(self, expect, code, analyze_at, potential, evaluate, volume) :\n",
    "        if self.check_exist(expect, code, analyze_at, evaluate):\n",
    "            print('duplicate', expect, code, analyze_at)\n",
    "        else :\n",
    "            cursor = self.conn.cursor()\n",
    "            print(expect,code,analyze_at,potential,volume,evaluate)\n",
    "            cursor.execute(\"INSERT INTO forecast (type, code, analyzeAt, potential, volume, evaluate, train) VALUES (%s, %s, %s, %s, %s, %s, %s)\",\n",
    "                           (expect, code, analyze_at, str(potential), volume, evaluate, TRAIN_CNT))\n",
    "            self.conn.commit()\n",
    "    def check_exist(self, expect, code, analyze_at, evaluate):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT count(*) as cnt FROM forecast WHERE type = %s AND code = %s AND analyzeAt = %s AND evaluate = %s AND train=%s\", (expect, code, analyze_at, evaluate, TRAIN_CNT))\n",
    "        cnt = cursor.fetchone()\n",
    "        return cnt[0] > 0\n",
    "    def get_volume(self, code, limit_at):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT count(*) as cnt FROM daily_stock WHERE code = %s AND date <= %s\", (code, limit_at))\n",
    "        cnt = cursor.fetchone()\n",
    "        return cnt[0]\n",
    "    def check_daily_target(self, target_at):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"select count(id) from data.daily_stock where date = %s\", (target_at))\n",
    "        cnt = cursor.fetchone()\n",
    "        return cnt[0] > 0\n",
    "    def get_last_date_at(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"select max(date) from data.daily_stock\")\n",
    "        last = cursor.fetchone()\n",
    "        return last[0]\n",
    "    def get_last_analyze_at(self, expect_type):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT max(analyzeAt) FROM data.forecast WHERE type = %s\", (expect_type))\n",
    "        last = cursor.fetchone()\n",
    "        return last[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_series_datas(expect, db, code_dates):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for code_date in code_dates:\n",
    "        items = db.get_items(code_date[0], code_date[1], TIME_STEP_SIZE + EVALUATE_SIZE)\n",
    "  \n",
    "        if len(items) < (EVALUATE_SIZE + TIME_STEP_SIZE):\n",
    "            break\n",
    "        X.append(np.array(items[:TIME_STEP_SIZE]))\n",
    "\n",
    "        st_purchase_inst = items[-(EVALUATE_SIZE + 1)][expect]\n",
    "        if st_purchase_inst == 0:\n",
    "            continue\n",
    "        for i in range(EVALUATE_SIZE, len(items) - EVALUATE_SIZE):\n",
    "            eval_inst = items[i][expect]\n",
    "            eval_bef = items[EVALUATE_SIZE-i][expect]\n",
    "            if eval_bef < eval_inst:\n",
    "                eval_bef = eval_inst           \n",
    "        \n",
    "        if (eval_bef - st_purchase_inst) / st_purchase_inst < -0.02: #percent ? cnt ? \n",
    "            Y.append((0., 0., 1.))\n",
    "        elif (eval_bef - st_purchase_inst) / st_purchase_inst > 0.03:\n",
    "            Y.append((1., 0., 0.))\n",
    "        else:\n",
    "            Y.append((0., 1., 0.))\n",
    "\n",
    "\n",
    "    arrX = np.array(X)    \n",
    "    meanX = np.mean(arrX, axis = 0)\n",
    "    stdX = np.std(arrX, axis = 0)\n",
    "    norX = (arrX - meanX) / stdX\n",
    "    norY = np.array(Y)\n",
    "    return norX, norY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_datas(expect, db, code_dates):    \n",
    "    np.random.seed()\n",
    "    np.random.shuffle(code_dates)\n",
    "\n",
    "    trX = list()\n",
    "    trY = list()\n",
    "    trX, trY = read_series_datas(expect, db, code_dates)\n",
    "    teX, teY = read_series_datas(expect, db, code_dates)\n",
    "\n",
    "    return trX, trY, teX, teY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def analyze(expect, code, limit):      \n",
    "    db = DBManager()\n",
    "    code_dates = db.get_codedates(code, limit)\n",
    "    tf.reset_default_graph()    \n",
    "    last = code_dates[-1][1]\n",
    "    trX, trY, teX, teY = read_datas(expect, db, code_dates)\n",
    "    if (len(trX) == 0):\n",
    "        return None\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, TIME_STEP_SIZE, INPUT_VEC_SIZE])\n",
    "    Y = tf.placeholder(tf.float32, [None, LABEL_SIZE])\n",
    "\n",
    "    W = init_weights([LSTM_SIZE, LABEL_SIZE])\n",
    "    B = init_weights([LABEL_SIZE])\n",
    "    py_x, state_size = model(code, X, W, B, LSTM_SIZE)\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "    predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "    # Launch the graph in a session\n",
    "    analyzed = None\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for loop in range(TRAIN_CNT):\n",
    "            for start, end in zip(range(0, len(trX), BATCH_SIZE), range(BATCH_SIZE, len(trX)+1, BATCH_SIZE)):\n",
    "                sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "\n",
    "            test_indices = np.arange(len(teY))\n",
    "            org = teY[test_indices]\n",
    "            res = sess.run(predict_op, feed_dict={X: teX[test_indices], Y: teY[test_indices]})\n",
    "            \n",
    "            if loop == TRAIN_CNT-1 :\n",
    "                result = np.mean(np.argmax(org, axis=1) == res)                \n",
    "                analyzed = {\"code\":code, \"per\":round(result, 2), \"date\":limit}\n",
    "    return analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(code, X, W, B, lstm_size):\n",
    "    XT = tf.transpose(X, [1, 0, 2]) \n",
    "    XR = tf.reshape(XT, [-1, lstm_size])\n",
    "    #ValueError: Tensor conversion requested dtype int32 for Tensor with dtype\n",
    "    #float32: 'Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)'\n",
    "#tf.cast(input_y, tf.float32) or tf.to_float(input_y).\n",
    "    X_split = tf.split(XR, num_or_size_splits=TIME_STEP_SIZE, axis=0)\n",
    "    with tf.variable_scope(code, reuse=False):\n",
    "        #cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        cell = tf.contrib.rnn.GRUCell(lstm_size)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell = cell, output_keep_prob = 0.5)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] * LSTM_DEPTH, state_is_tuple = True)\n",
    "    outputs, _states = tf.contrib.rnn.static_rnn(cell, X_split, dtype=tf.float32)\n",
    "\n",
    "    return tf.matmul(outputs[-1], W) + B, cell.state_size # State size to initialize the stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run(expect):\n",
    "    target_db = DBManager()\n",
    "    target_at = target_db.get_last_analyze_at(expect) - timedelta(days=1)\n",
    "    loop_size = timedelta(days=1)\n",
    "    limit_at = target_db.get_last_date_at().date()\n",
    "\n",
    "\n",
    "    while limit_at > target_at.date():    \n",
    "        target_at += loop_size\n",
    "        print(target_at)\n",
    "        db = DBManager()\n",
    "        codes = db.get_codes()\n",
    "        for code in codes :\n",
    "            if db.check_daily_target(target_at) is False:\n",
    "                print('non exist stock data')\n",
    "                continue\n",
    "            if db.check_exist(expect, code[0], target_at, EVALUATE_SIZE):\n",
    "                continue\n",
    "            analyzed = analyze(expect, code[0], target_at)\n",
    "            if analyzed is None:\n",
    "                print('none')\n",
    "                continue\n",
    "            db = DBManager()\n",
    "            volume = db.get_volume(analyzed[\"code\"], target_at)    \n",
    "            db.insert_result(expect, analyzed[\"code\"], target_at, analyzed[\"per\"], EVALUATE_SIZE, volume)        \n",
    "\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-06 00:00:00\n",
      "done\n",
      "2017-04-07 00:00:00\n",
      "3 A000030 2017-04-07 00:00:00 0.4 590 3\n",
      "3 A000050 2017-04-07 00:00:00 0.46 3292 3\n",
      "3 A000070 2017-04-07 00:00:00 0.36 3292 3\n",
      "3 A000080 2017-04-07 00:00:00 0.28 1852 3\n",
      "3 A000100 2017-04-07 00:00:00 0.27 3292 3\n",
      "3 A000120 2017-04-07 00:00:00 0.25 3292 3\n",
      "3 A000140 2017-04-07 00:00:00 0.46 3292 3\n",
      "3 A000150 2017-04-07 00:00:00 0.26 3292 3\n",
      "3 A000210 2017-04-07 00:00:00 0.3 3292 3\n",
      "3 A000230 2017-04-07 00:00:00 0.2 3292 3\n",
      "3 A000240 2017-04-07 00:00:00 0.53 3292 3\n",
      "3 A000270 2017-04-07 00:00:00 0.29 3292 3\n",
      "3 A000640 2017-04-07 00:00:00 0.31 3292 3\n",
      "3 A000660 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A000670 2017-04-07 00:00:00 0.44 3292 3\n",
      "3 A000720 2017-04-07 00:00:00 0.11 3292 3\n",
      "3 A000810 2017-04-07 00:00:00 0.45 3292 3\n",
      "3 A000880 2017-04-07 00:00:00 0.14 3292 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 A000990 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A001040 2017-04-07 00:00:00 0.3 3292 3\n",
      "3 A001060 2017-04-07 00:00:00 0.52 3292 3\n",
      "3 A001120 2017-04-07 00:00:00 0.27 3292 3\n",
      "3 A001230 2017-04-07 00:00:00 0.41 3292 3\n",
      "3 A001430 2017-04-07 00:00:00 0.78 3292 3\n",
      "3 A001450 2017-04-07 00:00:00 0.35 3292 3\n",
      "3 A001520 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A001680 2017-04-07 00:00:00 0.29 3292 3\n",
      "3 A001740 2017-04-07 00:00:00 0.67 3292 3\n",
      "3 A001780 2017-04-07 00:00:00 0.7 2440 3\n",
      "3 A001800 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A002240 2017-04-07 00:00:00 0.7 3292 3\n",
      "3 A002270 2017-04-07 00:00:00 0.4 3292 3\n",
      "3 A002350 2017-04-07 00:00:00 0.52 3292 3\n",
      "3 A002380 2017-04-07 00:00:00 0.27 3292 3\n",
      "3 A002620 2017-04-07 00:00:00 0.29 3292 3\n",
      "3 A002790 2017-04-07 00:00:00 0.73 3292 3\n",
      "3 A002960 2017-04-07 00:00:00 0.3 3292 3\n",
      "3 A003000 2017-04-07 00:00:00 0.51 3292 3\n",
      "3 A003030 2017-04-07 00:00:00 0.35 3292 3\n",
      "3 A003200 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A003240 2017-04-07 00:00:00 0.29 3292 3\n",
      "3 A003300 2017-04-07 00:00:00 0.72 3292 3\n",
      "3 A003410 2017-04-07 00:00:00 0.5 3292 3\n",
      "3 A003490 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A003520 2017-04-07 00:00:00 1.0 3292 3\n",
      "3 A003550 2017-04-07 00:00:00 0.46 3292 3\n",
      "3 A003570 2017-04-07 00:00:00 0.54 3292 3\n",
      "3 A003620 2017-04-07 00:00:00 0.38 3292 3\n",
      "3 A003850 2017-04-07 00:00:00 0.51 3292 3\n",
      "3 A003920 2017-04-07 00:00:00 0.2 3292 3\n",
      "3 A004000 2017-04-07 00:00:00 0.44 3292 3\n",
      "3 A004020 2017-04-07 00:00:00 0.36 3292 3\n",
      "3 A004170 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A004370 2017-04-07 00:00:00 0.36 3292 3\n",
      "3 A004490 2017-04-07 00:00:00 0.6 3292 3\n",
      "3 A004700 2017-04-07 00:00:00 0.22 3292 3\n",
      "3 A004710 2017-04-07 00:00:00 0.5 3292 3\n",
      "3 A004800 2017-04-07 00:00:00 0.54 3292 3\n",
      "3 A004990 2017-04-07 00:00:00 0.34 3292 3\n",
      "3 A005090 2017-04-07 00:00:00 0.15 3292 3\n",
      "3 A005180 2017-04-07 00:00:00 0.17 3292 3\n",
      "3 A005300 2017-04-07 00:00:00 0.31 3292 3\n",
      "3 A005380 2017-04-07 00:00:00 0.03 3292 3\n",
      "3 A005440 2017-04-07 00:00:00 0.0 3292 3\n",
      "duplicate 3 A005490 2017-04-07 00:00:00\n",
      "duplicate 3 A005610 2017-04-07 00:00:00\n",
      "duplicate 3 A005740 2017-04-07 00:00:00\n",
      "duplicate 3 A005830 2017-04-07 00:00:00\n",
      "duplicate 3 A005850 2017-04-07 00:00:00\n",
      "duplicate 3 A005930 2017-04-07 00:00:00\n",
      "duplicate 3 A005940 2017-04-07 00:00:00\n",
      "duplicate 3 A006260 2017-04-07 00:00:00\n",
      "duplicate 3 A006280 2017-04-07 00:00:00\n",
      "duplicate 3 A006400 2017-04-07 00:00:00\n",
      "duplicate 3 A006650 2017-04-07 00:00:00\n",
      "duplicate 3 A006800 2017-04-07 00:00:00\n",
      "duplicate 3 A007070 2017-04-07 00:00:00\n",
      "duplicate 3 A007210 2017-04-07 00:00:00\n",
      "duplicate 3 A007310 2017-04-07 00:00:00\n",
      "duplicate 3 A007570 2017-04-07 00:00:00\n",
      "duplicate 3 A007690 2017-04-07 00:00:00\n",
      "duplicate 3 A008060 2017-04-07 00:00:00\n",
      "duplicate 3 A008490 2017-04-07 00:00:00\n",
      "duplicate 3 A008560 2017-04-07 00:00:00\n",
      "duplicate 3 A008770 2017-04-07 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "3 A009240 2017-04-07 00:00:00 0.29 3292 3\n",
      "3 A009290 2017-04-07 00:00:00 0.52 3292 3\n",
      "3 A009420 2017-04-07 00:00:00 0.33 3292 3\n",
      "3 A009540 2017-04-07 00:00:00 0.44 3292 3\n",
      "3 A009830 2017-04-07 00:00:00 0.39 3292 3\n",
      "duplicate 3 A010060 2017-04-07 00:00:00\n",
      "duplicate 3 A010120 2017-04-07 00:00:00\n",
      "duplicate 3 A010130 2017-04-07 00:00:00\n",
      "none\n",
      "3 A010620 2017-04-07 00:00:00 0.52 3292 3\n",
      "3 A010780 2017-04-07 00:00:00 0.55 3292 3\n",
      "3 A010950 2017-04-07 00:00:00 0.48 3292 3\n",
      "duplicate 3 A011070 2017-04-07 00:00:00\n",
      "duplicate 3 A011170 2017-04-07 00:00:00\n",
      "duplicate 3 A011210 2017-04-07 00:00:00\n",
      "duplicate 3 A011780 2017-04-07 00:00:00\n",
      "duplicate 3 A011790 2017-04-07 00:00:00\n",
      "duplicate 3 A012330 2017-04-07 00:00:00\n",
      "duplicate 3 A012450 2017-04-07 00:00:00\n",
      "duplicate 3 A012750 2017-04-07 00:00:00\n",
      "duplicate 3 A014680 2017-04-07 00:00:00\n",
      "duplicate 3 A014820 2017-04-07 00:00:00\n",
      "duplicate 3 A014830 2017-04-07 00:00:00\n",
      "duplicate 3 A015760 2017-04-07 00:00:00\n",
      "duplicate 3 A017670 2017-04-07 00:00:00\n",
      "duplicate 3 A017800 2017-04-07 00:00:00\n",
      "duplicate 3 A018260 2017-04-07 00:00:00\n",
      "duplicate 3 A018880 2017-04-07 00:00:00\n",
      "duplicate 3 A019680 2017-04-07 00:00:00\n",
      "duplicate 3 A020000 2017-04-07 00:00:00\n",
      "duplicate 3 A021240 2017-04-07 00:00:00\n",
      "duplicate 3 A023530 2017-04-07 00:00:00\n",
      "duplicate 3 A024110 2017-04-07 00:00:00\n",
      "duplicate 3 A025540 2017-04-07 00:00:00\n",
      "duplicate 3 A025860 2017-04-07 00:00:00\n",
      "none\n",
      "duplicate 3 A028050 2017-04-07 00:00:00\n",
      "duplicate 3 A028260 2017-04-07 00:00:00\n",
      "duplicate 3 A029530 2017-04-07 00:00:00\n",
      "duplicate 3 A029780 2017-04-07 00:00:00\n",
      "duplicate 3 A030000 2017-04-07 00:00:00\n",
      "duplicate 3 A030200 2017-04-07 00:00:00\n",
      "none\n",
      "3 A033780 2017-04-07 00:00:00 0.5 3292 3\n",
      "3 A033920 2017-04-07 00:00:00 0.38 3292 3\n",
      "3 A034020 2017-04-07 00:00:00 0.45 3292 3\n",
      "3 A034120 2017-04-07 00:00:00 0.33 3292 3\n",
      "3 A034220 2017-04-07 00:00:00 0.05 3155 3\n",
      "3 A034730 2017-04-07 00:00:00 0.12 1835 3\n",
      "3 A035250 2017-04-07 00:00:00 0.55 3292 3\n",
      "none\n",
      "3 A036460 2017-04-07 00:00:00 0.42 3292 3\n",
      "3 A036570 2017-04-07 00:00:00 0.22 3292 3\n",
      "3 A036580 2017-04-07 00:00:00 0.39 3292 3\n",
      "3 A042660 2017-04-07 00:00:00 0.42 3292 3\n",
      "3 A042670 2017-04-07 00:00:00 0.14 3292 3\n",
      "3 A047040 2017-04-07 00:00:00 0.45 3292 3\n",
      "3 A047050 2017-04-07 00:00:00 0.0 3292 3\n",
      "3 A047810 2017-04-07 00:00:00 0.53 1428 3\n",
      "3 A049770 2017-04-07 00:00:00 0.32 3292 3\n",
      "3 A051600 2017-04-07 00:00:00 0.36 2310 3\n",
      "3 A051900 2017-04-07 00:00:00 0.09 3292 3\n",
      "3 A051910 2017-04-07 00:00:00 0.04 3292 3\n",
      "3 A052690 2017-04-07 00:00:00 0.5 1812 3\n",
      "3 A055550 2017-04-07 00:00:00 0.03 3292 3\n",
      "duplicate 3 A057050 2017-04-07 00:00:00\n",
      "duplicate 3 A060980 2017-04-07 00:00:00\n",
      "duplicate 3 A064350 2017-04-07 00:00:00\n",
      "duplicate 3 A064960 2017-04-07 00:00:00\n",
      "3 A066570 2017-04-07 00:00:00 0.5 3292 3\n",
      "duplicate 3 A069620 2017-04-07 00:00:00\n",
      "duplicate 3 A069960 2017-04-07 00:00:00\n",
      "duplicate 3 A071050 2017-04-07 00:00:00\n",
      "duplicate 3 A073240 2017-04-07 00:00:00\n",
      "duplicate 3 A078520 2017-04-07 00:00:00\n",
      "duplicate 3 A078930 2017-04-07 00:00:00\n",
      "duplicate 3 A079430 2017-04-07 00:00:00\n",
      "duplicate 3 A086280 2017-04-07 00:00:00\n",
      "duplicate 3 A086790 2017-04-07 00:00:00\n",
      "duplicate 3 A088350 2017-04-07 00:00:00\n",
      "duplicate 3 A090430 2017-04-07 00:00:00\n",
      "duplicate 3 A093050 2017-04-07 00:00:00\n",
      "duplicate 3 A093370 2017-04-07 00:00:00\n",
      "duplicate 3 A096770 2017-04-07 00:00:00\n",
      "3 A097230 2017-04-07 00:00:00 0.23 2381 3\n",
      "duplicate 3 A104700 2017-04-07 00:00:00\n",
      "duplicate 3 A105560 2017-04-07 00:00:00\n",
      "duplicate 3 A105630 2017-04-07 00:00:00\n",
      "duplicate 3 A108670 2017-04-07 00:00:00\n",
      "duplicate 3 A111770 2017-04-07 00:00:00\n",
      "duplicate 3 A114090 2017-04-07 00:00:00\n",
      "3 A115390 2017-04-07 00:00:00 0.41 1782 3\n",
      "duplicate 3 A138930 2017-04-07 00:00:00\n",
      "duplicate 3 A139480 2017-04-07 00:00:00\n",
      "duplicate 3 A145990 2017-04-07 00:00:00\n",
      "3 A161890 2017-04-07 00:00:00 0.36 1102 3\n",
      "3 A185750 2017-04-07 00:00:00 0.33 822 3\n",
      "none\n",
      "3 A204320 2017-04-07 00:00:00 0.3 621 3\n",
      "done\n",
      "2017-04-06 00:00:00\n",
      "done\n",
      "2017-04-07 00:00:00\n",
      "6 A000030 2017-04-07 00:00:00 0.11 590 3\n",
      "duplicate 6 A000050 2017-04-07 00:00:00\n",
      "duplicate 6 A000070 2017-04-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "expects = [3,6] ##open, high, low, close, volume, hold_foreign, st_purchase_inst\n",
    "for expect in expects:\n",
    "    run(expect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
